# Agentic AI Project

A sophisticated multi-agent AI system for code generation and document analysis using local language models with GPU acceleration support.

[![Python](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-supported-blue.svg)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸš€ Features

### Core Capabilities
- **Code Agent**: Generates, analyzes, and executes Python, C, and C++ code with intelligent error handling
- **Document Agent**: Advanced analysis of PDF, Excel, Word, and text files with RAG capabilities
- **Smart Routing**: Automatically routes queries to the most appropriate agent based on context
- **Multi-Modal Support**: Handle both text and file inputs seamlessly

### Technical Features
- **Local LLM Support**: Ollama integration (Mistral 7B, LLaMA 3.2, CodeLlama)
- **GPU Acceleration**: NVIDIA GPU support for faster inference
- **Scalable Architecture**: Client-server deployment with multiple frontend instances
- **RESTful API**: FastAPI backend for integration with external systems
- **Modern UI**: Streamlit-based interface with file upload and chat capabilities

### Deployment Options
- **Local Development**: Single-command setup for development
- **Docker Deployment**: Production-ready containerized deployment
- **Client-Server Architecture**: Scale with multiple client instances
- **GPU Support**: Automatic GPU detection and utilization

## ğŸƒ Quick Start

### Option 1: Local Development
Perfect for development and testing:

```bash
# Clone the repository
git clone <repository-url>
cd Agentic-AI-Project-BDL

# Automated setup (installs dependencies and starts Ollama)
python setup-dev.py

# Launch the application
python main.py
```

### Option 2: Docker Deployment
Recommended for production and easy scaling:

```bash
# Clone and navigate
git clone <repository-url>
cd Agentic-AI-Project-BDL

# Make deployment script executable
chmod +x deploy.sh

# Deploy main server with GPU support
./deploy.sh main

# (Optional) Deploy additional client instances
./deploy.sh client 1
./deploy.sh client 2
```

**Access Points:**
- **Web Interface**: `http://localhost:8501`
- **API Documentation**: `http://localhost:8000/docs`
- **Ollama API**: `http://localhost:11434`

### Option 3: API-Only Mode
For integration with existing applications:

```bash
# Start only the backend API
uvicorn backend.api_server:app --host 0.0.0.0 --port 8000

# Test the API
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Generate a Python function to calculate fibonacci numbers"}'
```

## ğŸ“‹ Requirements

### System Requirements
- **Python**: 3.12+ (for local development)
- **Memory**: 8GB+ RAM (16GB recommended for optimal performance)
- **Storage**: ~10GB free space (for models and containers)
- **OS**: Linux, macOS, or Windows with WSL2

### Optional but Recommended
- **Docker & Docker Compose**: For containerized deployment
- **NVIDIA GPU**: GTX 1060+ or RTX series for GPU acceleration
- **CUDA**: Version 11.0+ (automatically handled in Docker)

### Quick System Check
```bash
# Verify Python version
python --version

# Check available memory
free -h  # Linux/macOS
wmic computersystem get TotalPhysicalMemory  # Windows

# Verify GPU (if available)
nvidia-smi

# Check Docker installation
docker --version && docker compose version
```

## ğŸ—ï¸ Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API   â”‚    â”‚   LLM Provider  â”‚
â”‚  (Streamlit)    â”‚ â—„â”€â”€â–º â”‚   (FastAPI)     â”‚ â—„â”€â”€â–º â”‚   (Ollama)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chat Interface  â”‚    â”‚ Agent Router    â”‚    â”‚ GPU Accelerationâ”‚
â”‚ File Uploader   â”‚    â”‚ Code Agent      â”‚    â”‚ Model Storage   â”‚
â”‚ Response Stream â”‚    â”‚ Document Agent  â”‚    â”‚ API Endpoints   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent System
- **Agent Router**: Intelligently routes queries based on content analysis
- **Code Agent**: Specializes in code generation, analysis, and execution
- **Document Agent**: Handles file processing, summarization, and Q&A
- **Tool Integration**: Extensible tool system for specialized tasks

### Deployment Modes
1. **Standalone**: All components on a single machine
2. **Client-Server**: Centralized backend with multiple client frontends
3. **API-Only**: Backend service for external integration

## ğŸ“ Project Structure

```
Agentic-AI-Project-BDL/
â”œâ”€â”€ ğŸš€ main.py                    # Local application entry point
â”œâ”€â”€ âš™ï¸ setup-dev.py               # Development setup script  
â”œâ”€â”€ ğŸ³ deploy.sh                  # Docker deployment script
â”œâ”€â”€ ğŸ“– DEPLOYMENT_GUIDE.md        # Detailed deployment instructions
â”œâ”€â”€ ğŸ”§ backend/
â”‚   â”œâ”€â”€ ğŸ¤– agents/               # AI agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py        # Base agent class
â”‚   â”‚   â”œâ”€â”€ code_agent.py        # Code generation & analysis
â”‚   â”‚   â””â”€â”€ doc_agent.py         # Document processing
â”‚   â”œâ”€â”€ ğŸ› ï¸ tools/                # Specialized tools
â”‚   â”‚   â”œâ”€â”€ code_tools/          # Code execution & analysis
â”‚   â”‚   â””â”€â”€ doc_tools/           # Document processing tools
â”‚   â”œâ”€â”€ ğŸ§  llm_providers/        # LLM integrations
â”‚   â”‚   â”œâ”€â”€ ollama_provider.py   # Ollama integration
â”‚   â”‚   â””â”€â”€ tensorrt_provider.py # TensorRT-LLM support
â”‚   â”œâ”€â”€ ğŸ­ graphs/               # Workflow orchestration
â”‚   â”‚   â””â”€â”€ agent_controller.py  # LangGraph controller
â”‚   â”œâ”€â”€ ğŸ”— utils/                # Utilities
â”‚   â”‚   â”œâ”€â”€ memory.py            # Memory management
â”‚   â”‚   â””â”€â”€ router.py            # Request routing
â”‚   â””â”€â”€ ğŸŒ api_server.py         # FastAPI backend server
â”œâ”€â”€ ğŸ¨ frontend/
â”‚   â”œâ”€â”€ app.py                   # Standalone Streamlit app
â”‚   â”œâ”€â”€ client_app.py            # Client-server frontend
â”‚   â””â”€â”€ components/              # Reusable UI components
â”œâ”€â”€ âš™ï¸ config/                   # Configuration files
â”‚   â”œâ”€â”€ app_config.py            # Main application config
â”‚   â””â”€â”€ llm_config.py            # LLM-specific settings
â”œâ”€â”€ ğŸ³ deployment/               # Docker configurations
â”‚   â”œâ”€â”€ docker-compose.*.yml     # Various deployment configs
â”‚   â”œâ”€â”€ Dockerfile.*             # Service-specific Dockerfiles
â”‚   â””â”€â”€ scripts/                 # Deployment scripts
â””â”€â”€ ğŸ“Š data/                     # Data storage
    â””â”€â”€ uploads/                 # User uploaded files
```

### Key Components
- **Agent System**: Modular AI agents with specialized capabilities
- **Tool Integration**: Extensible tool system for code execution and document processing  
- **LLM Providers**: Support for multiple language model backends
- **Workflow Engine**: LangGraph-based orchestration for complex tasks
- **API Layer**: RESTful API for external integrations

## âš™ï¸ Configuration & Customization

### Model Selection

The system supports multiple LLM models via Ollama. Edit `config/app_config.py`:

```python
LLM_CONFIG = {
    "provider": "ollama",
    "model": "mistral:7b",        # Default: balanced performance
    "temperature": 0.7,           # Creativity level (0.0-1.0)
    "max_tokens": 1000           # Response length limit
}
```

**Available Models:**
| Model | Size | Best For | Performance |
|-------|------|----------|-------------|
| `mistral:7b` | ~4.1GB | General tasks, balanced | â­â­â­â­ |
| `llama3.2:1b` | ~1.3GB | Quick responses, low memory | â­â­â­ |
| `llama3.2:3b` | ~2.0GB | Good balance | â­â­â­â­ |
| `codellama:7b` | ~3.8GB | Code generation | â­â­â­â­â­ |

### Install a New Model
```bash
# Pull model (will download automatically)
ollama pull llama3.2:3b

# Update config and restart application
# Local: python main.py
# Docker: ./deploy.sh main
```

### Environment Variables
```bash
# Application settings
export STREAMLIT_PORT=8501
export API_PORT=8000
export OLLAMA_HOST=localhost:11434

# GPU settings (optional)
export CUDA_VISIBLE_DEVICES=0
export NVIDIA_VISIBLE_DEVICES=all

# Deployment mode
export SERVER_TYPE=standalone  # or 'client' for client-server
export MAIN_SERVER_URL=http://localhost:8000  # for client mode
```

### Advanced Configuration
For production deployments, see the [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for:
- Multi-client scaling
- GPU optimization 
- Network configuration
- Security settings

## ğŸ’¡ Usage Examples

### Code Generation
```
User: "Create a Python function to analyze sentiment of text using transformers"

AI Response:
âœ… Code generated and tested
âœ… Dependencies identified  
âœ… Example usage provided
âœ… Error handling included
```

### Document Analysis
```
Upload: business_report.pdf (2.3MB)
Query: "What are the key financial metrics mentioned?"

AI Response:
ğŸ“Š Revenue: $2.4M (â†‘15% YoY)
ğŸ“ˆ Profit Margin: 18.5%
ğŸ’° Cash Flow: $450K positive
ğŸ“‹ Full summary with page references
```

### Multi-Agent Workflow
```
Query: "Analyze this CSV file and create a visualization script"

Process:
1. ğŸ“„ Document Agent: Parses CSV structure  
2. ğŸ”„ Router: Determines need for both agents
3. ğŸ’» Code Agent: Generates matplotlib script
4. ğŸ¯ Result: Working visualization code + data insights
```

### API Integration
```python
import requests

# Chat with the AI
response = requests.post("http://localhost:8000/chat", json={
    "message": "Generate a REST API using FastAPI",
    "agent": "code"  # Optional: specify agent
})

print(response.json()["response"])
```

## ğŸ› ï¸ Development Workflow

### Setting Up Development Environment
```bash
# Clone and setup
git clone <repository-url>
cd Agentic-AI-Project-BDL

# Install in development mode
pip install -e .
pip install -r requirements.txt

# Install pre-commit hooks (optional)
pre-commit install

# Start development server with auto-reload
python main.py --dev
```

### Adding New Agents
```python
# Create new agent in backend/agents/
from .base_agent import BaseAgent

class CustomAgent(BaseAgent):
    def process_request(self, context):
        # Your agent logic here
        return response

# Register in agent_controller.py
agents = {
    "custom": CustomAgent(),
    # ... existing agents
}
```

### Testing
```bash
# Run all tests
python -m pytest tests/

# Run specific test category
python -m pytest tests/test_agents.py
python -m pytest tests/test_api.py

# Test with coverage
python -m pytest --cov=backend tests/
```

## ğŸ“Š Monitoring & Logs

### Health Checks
```bash
# Check all services
curl http://localhost:8000/health      # Backend API
curl http://localhost:11434/api/tags   # Ollama models
curl http://localhost:8501             # Frontend

# Docker container status
docker ps
docker stats
```

### Log Monitoring
```bash
# View application logs
docker logs agentic-backend-main -f
docker logs agentic-ollama-main -f
docker logs agentic-frontend-client-1 -f

# Local development logs
tail -f logs/app.log
tail -f logs/errors.log
```

### Performance Monitoring
```bash
# GPU usage (if available)
nvidia-smi -l 1

# Memory usage
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

# Network activity
netstat -i
```

## ğŸ”§ Troubleshooting

### Common Issues

**âŒ "Ollama not found" or Connection Refused**
```bash
# Install Ollama (if not using Docker)
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve

# Or restart Docker services
docker restart agentic-ollama-main
```

**âŒ Port Already in Use**
```bash
# Find what's using the port
sudo lsof -i :8501
sudo netstat -tulpn | grep :8000

# Use different ports
export STREAMLIT_PORT=8502
export API_PORT=8002

# Or kill the process
sudo kill -9 <PID>
```

**âŒ GPU Not Detected or Out of Memory**
```bash
# Check GPU status
nvidia-smi

# Use smaller model
ollama pull llama3.2:1b

# Reduce token limit in config/app_config.py
LLM_CONFIG["max_tokens"] = 500

# Clear GPU memory
sudo pkill -f ollama
docker restart agentic-ollama-main
```

**âŒ Model Download Fails**
```bash
# Manual model pull
docker exec -it agentic-ollama-main ollama pull mistral:7b

# Check available space
df -h

# Alternative model sources
ollama pull llama3.2:1b  # Smaller alternative
```

**âŒ Frontend Won't Load**
```bash
# Check Streamlit status
docker logs agentic-frontend-client-1

# Restart frontend
docker restart agentic-frontend-client-1

# Check browser console for errors
# Try incognito mode (clear cache)
```

**âŒ File Upload Issues**
```bash
# Check permissions
ls -la data/uploads/
chmod 755 data/uploads/

# Check disk space
du -sh data/
df -h

# Clear upload cache
rm -rf data/uploads/*.tmp
```

### Getting Help
- ğŸ“– Check [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for detailed setup
- ğŸ› Check logs: `docker logs <container-name>`
- ğŸ’¬ Open an issue with:
  - Error message
  - System info (`python --version`, `docker --version`)
  - Steps to reproduce

## ğŸ¤ Contributing

### Ways to Contribute
- ğŸ› **Bug Reports**: Found an issue? Open a detailed bug report
- ğŸ’¡ **Feature Requests**: Have an idea? Suggest new features
- ğŸ“ **Documentation**: Improve docs, add examples
- ğŸ§ª **Testing**: Add tests, improve coverage
- ğŸ› ï¸ **Code**: Fix bugs, implement features

### Development Process
1. **Fork** the repository
2. **Create** a feature branch:
   ```bash
   git checkout -b feature/awesome-new-feature
   ```
3. **Develop** with tests:
   ```bash
   # Make your changes
   # Add tests in tests/
   python -m pytest tests/
   ```
4. **Document** your changes:
   ```bash
   # Update docstrings
   # Update README if needed
   # Add example usage
   ```
5. **Submit** a pull request:
   - Clear description of changes
   - Link to any related issues
   - Include screenshots if UI changes

### Code Style
```bash
# Format code
black backend/ frontend/
isort backend/ frontend/

# Lint code  
flake8 backend/ frontend/
mypy backend/

# Run pre-commit hooks
pre-commit run --all-files
```

## ğŸ“š Additional Resources

- ğŸ“– **[Deployment Guide](DEPLOYMENT_GUIDE.md)**: Detailed deployment instructions
- ğŸ”§ **[API Documentation](http://localhost:8000/docs)**: Interactive API docs (when server is running)
- ğŸ¤– **[Ollama Models](https://ollama.ai/library)**: Browse available models
- ğŸ³ **[Docker Hub](https://hub.docker.com/r/ollama/ollama)**: Ollama Docker images

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ using Python, Docker, and AI**

[â­ Star this repo](../../stargazers) â€¢ [ğŸ› Report Bug](../../issues) â€¢ [ğŸ’¡ Request Feature](../../issues)

</div>