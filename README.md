<div align="center">

# ğŸš€ Agentic AI Project

### *Next-Generation Multi-Agent AI System*

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![NVIDIA](https://img.shields.io/badge/NVIDIA-GPU_Supported-76B900?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-toolkit)

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://streamlit.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama&logoColor=white)](https://ollama.ai/)
[![TensorRT](https://img.shields.io/badge/TensorRT-76B900?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/tensorrt)

![GitHub Issues](https://img.shields.io/github/issues/username/agentic-ai-project?style=flat-square&color=red)
![GitHub Stars](https://img.shields.io/github/stars/username/agentic-ai-project?style=flat-square&color=yellow)
![GitHub Forks](https://img.shields.io/github/forks/username/agentic-ai-project?style=flat-square&color=blue)
![Last Commit](https://img.shields.io/github/last-commit/username/agentic-ai-project?style=flat-square&color=green)

---

ğŸ¤– **A powerful agentic AI system that combines code generation, document processing, and intelligent task routing using advanced language models.**

*Built with cutting-edge AI agents, GPU acceleration, and enterprise-ready scalability*

![Demo](https://via.placeholder.com/800x400/0d1117/58a6ff?text=ğŸš€+Agentic+AI+System+ğŸ¤–%0ACode+Generation+â€¢+Document+Analysis+â€¢+Task+Automation)

### ğŸŒŸ **Star this repo** if you find it useful! â­

[ğŸš€ **Quick Start**](#-quick-start) â€¢ 
[ğŸ“– **Documentation**](#-table-of-contents) â€¢ 
[ğŸ”§ **Model Config**](#-model-configuration) â€¢ 
[ğŸ—ï¸ **Architecture**](#ï¸-architecture) â€¢ 
[ğŸ’¡ **Examples**](#-usage-examples) â€¢ 
[ğŸ› **Issues**](https://github.com/username/agentic-ai-project/issues)

</div>

## âœ¨ Key Features

<div align="center">

| Feature | Description | Status |
|---------|-------------|--------|
| ğŸ¤– **Multi-Agent Architecture** | Specialized agents for code generation and document analysis | âœ… Active |
| ğŸš€ **GPU Acceleration** | NVIDIA GPU support for optimal performance | âœ… Active |
| ğŸ“„ **Document Processing** | PDF, text, and Excel file analysis with RAG capabilities | âœ… Active |
| ğŸ’» **Code Execution** | Safe code generation and execution environment | âœ… Active |
| ğŸ”„ **Distributed Deployment** | Support for main server + multiple client configurations | âœ… Active |
| ğŸ¯ **Intelligent Routing** | Automatic task routing to appropriate agents | âœ… Active |
| ğŸ³ **Docker Ready** | One-command deployment with Docker Compose | âœ… Active |
| ğŸ”§ **Model Flexibility** | Easy switching between different Ollama models | âœ… Active |

</div>

### ğŸ¯ **What Makes This Special?**

- ğŸ§  **Smart Agent Selection**: Automatically routes tasks to the most suitable AI agent
- âš¡ **Lightning Fast**: GPU-accelerated inference with TensorRT optimization
- ğŸ”’ **Production Ready**: Scalable architecture with Docker containerization
- ğŸ“Š **Real-time Processing**: Live document analysis and code execution
- ğŸŒ **Multi-Client Support**: Serve multiple users from a single GPU server
- ğŸ”„ **Model Agnostic**: Switch between Llama, Mistral, CodeLlama, and more

## ğŸš€ Quick Start

<div align="center">

### ğŸ”¥ **Get Started in 2 Commands!**

**Local Development:**
```bash
git clone <repository-url> && cd Agentic-AI-Project-BDL
python dev.py  # Auto-detects TensorRT or Ollama
```

**Docker Deployment:**
```bash
git clone <repository-url> && cd Agentic-AI-Project-BDL
chmod +x deploy.sh && ./deploy.sh main
```

</div>

### âš¡ Prerequisites

<details>
<summary>ğŸ“‹ <strong>System Requirements (Click to expand)</strong></summary>

#### ğŸ”§ **Required**
- ğŸ³ [Docker](https://docs.docker.com/get-docker/) (20.10+) and [Docker Compose](https://docs.docker.com/compose/install/) (2.0+)
- ğŸ–¥ï¸ Linux/Windows/macOS (x64)
- ğŸ’¾ **8GB+ RAM** (16GB recommended)
- ğŸ’¿ **10GB+ free disk space**

#### ğŸ® **Recommended**
- ğŸ”¥ NVIDIA GPU with [CUDA drivers](https://developer.nvidia.com/cuda-downloads) (11.8+)
- âš¡ **RTX 3060** or better (8GB+ VRAM)
- ğŸƒâ€â™‚ï¸ **16+ CPU cores** for optimal performance

#### ğŸ§ª **Development Options**

<details>
<summary>ğŸ”§ <strong>Advanced Local Setup (Click to expand)</strong></summary>

```bash
# Force specific provider
python dev.py --tensorrt    # Use TensorRT-LLM only
python dev.py --ollama      # Use Ollama only

# Install dependencies only
python dev.py --setup       # Setup without starting

# Manual Streamlit (after setup)
cd frontend && streamlit run app.py
```

**ğŸ¯ Provider Priority:**
1. **TensorRT-LLM** (localhost:8000) - Best performance
2. **Ollama** (localhost:11434) - Auto-installed fallback

</details>

</details>

### ğŸ¯ **Deployment Options**

<div align="center">

| Deployment Type | Command | Best For | Resources |
|----------------|---------|----------|-----------|
| ğŸ  **Single Server** | `./deploy.sh main` | Development, Testing | 1 GPU, 8GB RAM |
| ğŸŒ **Multi-Client** | `./deploy.sh client 1` | Teams, Production | Shared GPU |
| ğŸ”§ **Manual Setup** | See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) | Custom Configs | Variable |

</div>

### âš¡ **One-Command Setup**

<div align="center">

**ğŸ–¥ï¸ Local Development** | **ğŸ³ Docker Deployment**

</div>

#### ğŸ–¥ï¸ **Local Development (Simplified)**
```bash
# 1ï¸âƒ£ Clone and navigate
git clone <repository-url>
cd Agentic-AI-Project-BDL

# 2ï¸âƒ£ Complete setup with dependencies, Ollama, and models
python setup-dev.py
```

#### ğŸ³ **Docker Deployment (Production)**
```bash
# 1ï¸âƒ£ Main server deployment
chmod +x deploy.sh && ./deploy.sh main

# 2ï¸âƒ£ Additional clients (optional)
./deploy.sh client 1  # Port 8501
./deploy.sh client 2  # Port 8502
```

<div align="center">

### ğŸŒ **Access Your AI System**

ğŸ‰ **Main Application**: [`http://localhost:8501`](http://localhost:8501)  
ğŸ”§ **API Documentation**: [`http://localhost:8000/docs`](http://localhost:8000/docs)  
ğŸ¤– **Ollama API**: [`http://localhost:11434`](http://localhost:11434)

**ğŸš€ Ready to start creating with AI!**

</div>

## ğŸ“‹ Table of Contents

<div align="center">

**ğŸ“– Navigation Guide**

</div>

| Section | Description | ğŸ”— |
|---------|-------------|---|
| ğŸ”§ **[Model Configuration](#-model-configuration)** | Switch between AI models | ğŸ¯ |
| ğŸ—ï¸ **[Architecture](#ï¸-architecture)** | System design & components | ğŸ›ï¸ |
| ğŸš€ **[Deployment Options](#-deployment-options)** | Setup & scaling guides | ğŸ›ï¸ |
| ğŸ’¡ **[Usage Examples](#-usage-examples)** | Real-world use cases | ğŸª |
| ğŸ› ï¸ **[Configuration Files](#ï¸-configuration-files)** | Customize your setup | âš™ï¸ |
| ğŸ› **[Troubleshooting](#-troubleshooting)** | Common fixes & solutions | ğŸ”§ |
| ğŸ‘¨â€ğŸ’» **[Development](#-development)** | Contributing & extending | ğŸš§ |
| ğŸ“Š **[Performance](#-performance)** | Optimization tips | âš¡ |
| ğŸ¤ **[Contributing](#-contributing)** | Join the community | ğŸ’ª |
| ğŸ“„ **[License](#-license)** | MIT License details | ğŸ“‹ |

## ğŸ”§ Model Configuration

### ğŸ¯ Switching Ollama Models

The system is configured to use **Mistral 7B** by default. You can switch to different Ollama models by following these steps:

#### ğŸ“Š Available Models

| Model | Size | Memory | Performance | Use Case |
|-------|------|--------|-------------|----------|
| `mistral:7b` â­ | 4.1GB | 8GB+ | âš¡âš¡âš¡ | **Default** - Balanced performance |
| `llama3.2:1b` | 1.3GB | 4GB+ | âš¡âš¡âš¡âš¡ | Lightweight, faster responses |
| `llama3.2:3b` | 2.0GB | 6GB+ | âš¡âš¡âš¡ | Good balance of speed and quality |
| `llama3.1:8b` | 4.7GB | 8GB+ | âš¡âš¡ | High quality responses |
| `codellama:7b` | 3.8GB | 8GB+ | âš¡âš¡âš¡ | ğŸ Specialized for code tasks |
| `phi3:mini` | 2.3GB | 4GB+ | âš¡âš¡âš¡âš¡ | ğŸ¢ Microsoft's efficient model |
| `gemma2:2b` | 1.6GB | 4GB+ | âš¡âš¡âš¡âš¡ | ğŸ” Google's lightweight model |

#### ğŸ”§ Method 1: Update Configuration Files (Recommended)

<details>
<summary>ğŸ¯ <strong>Click to expand step-by-step instructions</strong></summary>

1. **Update the main configuration:**

```bash
# Edit the main config file
nano config/app_config.py
```

Change the model in `LLM_CONFIG`:
```python
LLM_CONFIG = {
    "provider": "tensorrt",
    "model": "llama3.2:3b",  # ğŸ‘ˆ Change this line
    "temperature": 0.7,
    "max_tokens": 1000
}
```

2. **Update the Ollama provider default:**

```bash
# Edit the Ollama provider
nano backend/llm_providers/ollama_provider.py
```

Change the default model:
```python
def __init__(
    self, 
    model_name: str = "llama3.2:3b",  # ğŸ‘ˆ Change this line
    temperature: float = 0.7,
    max_tokens: int = 2000,
    base_url: str = None
):
```

3. **Update Docker Compose files:**

```bash
# Update all Docker Compose files at once
sed -i 's/LLM_MODEL=mistral:7b/LLM_MODEL=llama3.2:3b/g' deployment/docker-compose.*.yml
```

4. **Update deployment script:**

```bash
# Edit deploy.sh
nano deploy.sh
```

Change the model pull command:
```bash
# From:
docker exec agentic-ollama-main ollama pull mistral:7b

# To:
docker exec agentic-ollama-main ollama pull llama3.2:3b
```

</details>

#### âš¡ Method 2: Environment Variables (Quick & Temporary)

```bash
# Set environment variable before deployment
export LLM_MODEL="llama3.2:3b"

# Deploy with the new model
./deploy.sh main
```

#### ğŸ”„ Method 3: Runtime Model Switch

<details>
<summary>ğŸ”§ <strong>Switch models on running system</strong></summary>

1. **Pull the new model:**
```bash
# Connect to Ollama container
docker exec -it agentic-ollama-main bash

# Pull new model
ollama pull llama3.2:3b

# List available models
ollama list

# Exit container
exit
```

2. **Restart with new model:**
```bash
# Stop current deployment
cd deployment
docker compose -f docker-compose.main-server-gpu.yml down

# Set new model and restart
export LLM_MODEL="llama3.2:3b"
docker compose -f docker-compose.main-server-gpu.yml up -d
```

</details>

#### ğŸ›ï¸ Method 4: Multiple Models (Advanced)

```bash
# Pull multiple models for switching
docker exec agentic-ollama-main ollama pull mistral:7b
docker exec agentic-ollama-main ollama pull llama3.2:3b
docker exec agentic-ollama-main ollama pull codellama:7b

# List all available models
docker exec agentic-ollama-main ollama list

# Switch by updating environment variable and restarting
```

### Model-Specific Configurations

#### For Code-Heavy Tasks (Recommended: CodeLlama)

```python
LLM_CONFIG = {
    "provider": "tensorrt",
    "model": "codellama:7b",
    "temperature": 0.1,  # Lower temperature for code
    "max_tokens": 2000   # More tokens for code generation
}
```

#### For Resource-Constrained Environments

```python
LLM_CONFIG = {
    "provider": "tensorrt", 
    "model": "llama3.2:1b",
    "temperature": 0.7,
    "max_tokens": 500    # Reduced for faster responses
}
```

#### For Document Analysis Tasks

```python
LLM_CONFIG = {
    "provider": "tensorrt",
    "model": "mistral:7b",
    "temperature": 0.3,  # Lower temperature for factual responses
    "max_tokens": 1500   # More tokens for detailed analysis
}
```

### Verifying Model Switch

After switching models, verify the change:

1. **Check backend logs:**
```bash
docker logs agentic-backend-main | grep -i model
```

2. **Test API endpoint:**
```bash
curl http://localhost:8000/health
```

3. **Check Ollama directly:**
```bash
curl http://localhost:11434/api/tags
```

4. **Test in the web interface:**
   - Go to `http://localhost:8501`
   - Send a message
   - Check the sidebar for agent/model information

## ğŸ—ï¸ Architecture

### ğŸ”§ System Components

```mermaid
graph TB
    subgraph "Client Layer"
        C1[Client App 1<br/>Streamlit]
        C2[Client App 2<br/>Streamlit]
        C3[Client App N<br/>Streamlit]
    end
    
    subgraph "Main Server"
        API[FastAPI Backend<br/>Port 8000]
        ROUTER[Intelligent Router]
        CA[Code Agent]
        DA[Document Agent]
    end
    
    subgraph "AI Layer"
        OLLAMA[Ollama API<br/>Port 11434]
        GPU[GPU Acceleration<br/>NVIDIA CUDA]
    end
    
    subgraph "Tools"
        CE[Code Executor]
        DP[Document Processor]
        PDF[PDF Loader]
    end
    
    C1 --> API
    C2 --> API
    C3 --> API
    API --> ROUTER
    ROUTER --> CA
    ROUTER --> DA
    CA --> CE
    DA --> DP
    DA --> PDF
    CA --> OLLAMA
    DA --> OLLAMA
    OLLAMA --> GPU
```

### ğŸ¤– Agent System

| Component | Function | Specialization |
|-----------|----------|----------------|
| **ğŸ§­ Router** | Task Analysis & Routing | Intelligently routes requests to appropriate agents |
| **ğŸ’» Code Agent** | Code Generation & Analysis | Python, JavaScript, algorithms, debugging |
| **ğŸ“„ Document Agent** | Document Processing | PDF analysis, summarization, Q&A with RAG |
| **âš™ï¸ Code Executor** | Safe Code Execution | Sandboxed Python code execution |
| **ğŸ“ Document Processor** | File Processing | Multi-format document parsing and analysis |

## ğŸš€ Deployment Options

## ğŸš€ Deployment Options

### ğŸ  Option 1: Single Server (Simple)
```bash
./deploy.sh main
```
**Perfect for:**
- âœ… Development and testing
- âœ… Single-user environments
- âœ… Full feature access

**Requirements:**
- ğŸ® GPU recommended
- ğŸ’¾ 8GB+ RAM
- ğŸ’¿ Single instance

### ğŸŒ Option 2: Distributed Architecture (Scalable)
```bash
# 1. Deploy main server (once)
./deploy.sh main

# 2. Deploy multiple clients (as needed)
./deploy.sh client 1  # Port 8501
./deploy.sh client 2  # Port 8502  
./deploy.sh client 3  # Port 8503
```
**Perfect for:**
- âœ… Multi-user environments
- âœ… Team collaboration
- âœ… Resource sharing
- âœ… Horizontal scaling

**Architecture:**
- ğŸ¢ One powerful main server (GPU-enabled)
- ğŸ‘¥ Multiple lightweight client interfaces
- ğŸ”„ Shared AI model resources

### ğŸ”§ Option 3: Manual Deployment (Advanced)
See [ğŸ“– DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for detailed manual setup instructions.

## ğŸ’¡ Usage Examples

<div align="center">

### ğŸ¯ **Real-World Use Cases**

*See the AI agents in action with these practical examples*

</div>

### ğŸ’» Code Generation & Execution

<details>
<summary>ğŸ <strong>Example: Fibonacci Calculator (Click to expand)</strong></summary>

**Input:**
```
"Create a Python function to calculate fibonacci numbers and test it with n=10"
```

**System Flow:**
```mermaid
graph LR
    A[ğŸ‘¤ User Input] --> B[ğŸ§­ Router] 
    B --> C[ğŸ’» Code Agent]
    C --> D[ğŸ¤– LLM Generation]
    D --> E[âš™ï¸ Code Executor]
    E --> F[ğŸ“Š Results Display]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#e0f2f1
```

**Generated Output:**
```python
def fibonacci(n):
    """Calculate the nth Fibonacci number efficiently."""
    if n <= 1:
        return n
    
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b

# Test execution
result = fibonacci(10)
print(f"Fibonacci(10) = {result}")  # Output: 55
```

**âœ… Results:**
- âœ… Generated optimized Fibonacci function
- âœ… Executed with test case (n=10 â†’ 55)
- âœ… Performance analysis included
- âœ… Code explanation provided

</details>

### ğŸ“„ Document Analysis & Q&A

<details>
<summary>ğŸ“Š <strong>Example: PDF Report Analysis (Click to expand)</strong></summary>

**Input:**
- ğŸ“ Upload: `quarterly_report_Q4_2024.pdf`
- ğŸ’¬ Query: `"Summarize the key findings and create an executive summary"`

**System Flow:**
```mermaid
graph LR
    A[ğŸ“ PDF Upload] --> B[ğŸ§­ Router]
    B --> C[ğŸ“„ Doc Agent]
    C --> D[ğŸ“Š RAG Processing]
    D --> E[ğŸ¤– LLM Analysis]
    E --> F[ğŸ“‹ Summary Output]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff8e1
    style E fill:#fce4ec
    style F fill:#e0f2f1
```

**Generated Output:**
```markdown
# Executive Summary - Q4 2024

## ğŸ¯ Key Findings
- **Revenue Growth**: 15% increase over Q3 2024
- **Market Expansion**: Entered 3 new international markets
- **Customer Satisfaction**: 94% retention rate

## ğŸ“Š Financial Highlights
- Total Revenue: $2.4M (+15% QoQ)
- Operating Margin: 23% (+2% QoQ)
- Cash Flow: $850K positive

## ğŸš€ Strategic Initiatives
1. AI/ML product development (+30% investment)
2. International expansion (EU, APAC)
3. Customer success program launch
```

**âœ… Results:**
- âœ… Extracted 47 pages of content
- âœ… Generated executive summary
- âœ… Identified key metrics & trends
- âœ… Interactive Q&A enabled

</details>

### ğŸ”„ Multi-Agent Collaboration

<details>
<summary>ğŸ“ˆ <strong>Example: Excel Data Dashboard (Click to expand)</strong></summary>

**Input:**
- ğŸ“Š Upload: `sales_data_2024.xlsx`
- ğŸ’¬ Query: `"Analyze this dataset and create a visualization dashboard with Python"`

**System Flow:**
```mermaid
graph TB
    A[ğŸ“Š Excel Upload] --> B[ğŸ§­ Router]
    B --> C[ğŸ“„ Doc Agent]
    C --> D[ğŸ“Š Data Analysis]
    D --> E[ğŸ”„ Agent Handoff]
    E --> F[ğŸ’» Code Agent]
    F --> G[ğŸ Dashboard Code]
    G --> H[âš™ï¸ Execution]
    H --> I[ğŸ“ˆ Live Dashboard]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff8e1
    style E fill:#ffebee
    style F fill:#e8f5e8
    style G fill:#f1f8e9
    style H fill:#fce4ec
    style I fill:#e0f2f1
```

**Generated Python Dashboard:**
```python
import pandas as pd
import plotly.graph_objects as go
import streamlit as st

# Data analysis results
df = pd.read_excel('sales_data_2024.xlsx')
monthly_sales = df.groupby('month')['revenue'].sum()

# Interactive dashboard
fig = go.Figure()
fig.add_trace(go.Scatter(
    x=monthly_sales.index,
    y=monthly_sales.values,
    mode='lines+markers',
    name='Monthly Revenue'
))

st.plotly_chart(fig, use_container_width=True)
```

**âœ… Results:**
- âœ… Analyzed 12 months of sales data
- âœ… Generated interactive Plotly dashboard
- âœ… Identified seasonal trends (+25% Q4 boost)
- âœ… Created executable Python code
- âœ… Live dashboard deployed

</details>

### ğŸ› ï¸ **Development Workflow**

<div align="center">

**ğŸ”„ Typical AI-Powered Workflow**

</div>

```mermaid
graph TB
    subgraph "ğŸ¯ User Journey"
        A[ğŸ“¤ Upload Documents<br/>or Describe Task] 
        B[ğŸ¤– AI Processing<br/>Auto Agent Selection]
        C[ğŸ“Š Review Results<br/>Code/Analysis/Content]
        D[ğŸ”„ Iterate & Refine<br/>Follow-up Questions]
        E[â¬‡ï¸ Export & Deploy<br/>Download/Execute]
    end
    
    A --> B
    B --> C
    C --> D
    D --> B
    C --> E
    
    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#fff8e1
    style D fill:#f3e5f5
    style E fill:#e0f2f1
```

### ğŸª **Live Demo Examples**

<div align="center">

| Use Case | Demo | Description |
|----------|------|-------------|
| ğŸ **Code Generation** | [Try it â†’](http://localhost:8501) | Generate, test, and execute Python code |
| ğŸ“„ **Document Q&A** | [Try it â†’](http://localhost:8501) | Upload PDFs and ask questions |
| ğŸ“Š **Data Analysis** | [Try it â†’](http://localhost:8501) | Process Excel files and create visualizations |
| ğŸ”§ **API Integration** | [Docs â†’](http://localhost:8000/docs) | Integrate with your applications |

</div>

## ğŸ› ï¸ Configuration Files

<div align="center">

### âš™ï¸ **System Configuration Overview**

*Customize your AI system with these key configuration files*

</div>

| File | Purpose | Key Settings | ğŸ”§ |
|------|---------|--------------|---|
| ğŸ“ `config/app_config.py` | Main application config | Model selection, API settings | [Edit](#) |
| ğŸ¤– `config/llm_config.py` | LLM-specific settings | Temperature, tokens, providers | [Edit](#) |
| ğŸ³ `deployment/docker-compose.*.yml` | Container configurations | Ports, volumes, environments | [Edit](#) |
| ğŸš€ `deploy.sh` | Automated deployment script | Model pulling, container management | [Edit](#) |

### ğŸ”§ **Quick Configuration Examples**

<details>
<summary>âš™ï¸ <strong>Model Performance Tuning (Click to expand)</strong></summary>

#### ğŸ¯ **High-Quality Responses**
```python
# config/app_config.py
LLM_CONFIG = {
    "provider": "tensorrt",
    "model": "llama3.1:8b",
    "temperature": 0.2,     # Lower = more focused
    "max_tokens": 2000,     # More detailed responses
    "top_p": 0.9           # Nucleus sampling
}
```

#### âš¡ **Fast Performance**
```python
# config/app_config.py
LLM_CONFIG = {
    "provider": "tensorrt", 
    "model": "llama3.2:1b",
    "temperature": 0.7,
    "max_tokens": 500,      # Shorter, faster responses
    "top_p": 0.95
}
```

#### ğŸ’» **Code-Optimized**
```python
# config/app_config.py
LLM_CONFIG = {
    "provider": "tensorrt",
    "model": "codellama:7b",
    "temperature": 0.1,     # Very focused for code
    "max_tokens": 3000,     # Longer code blocks
    "top_p": 0.8
}
```

</details>

<details>
<summary>ğŸ³ <strong>Docker Environment Variables (Click to expand)</strong></summary>

#### ğŸŒ **Environment Configuration**
```yaml
# deployment/docker-compose.main-server-gpu.yml
environment:
  - LLM_MODEL=mistral:7b         # Model selection
  - CUDA_VISIBLE_DEVICES=0       # GPU assignment
  - OLLAMA_HOST=0.0.0.0:11434   # API host
  - PYTHONPATH=/app             # Python path
  - LOG_LEVEL=INFO              # Logging level
```

#### ğŸ”§ **Resource Limits**
```yaml
# Docker resource configuration
deploy:
  resources:
    limits:
      memory: 16G               # RAM limit
      cpus: '8'                # CPU cores
    reservations:
      devices:
        - driver: nvidia        # GPU reservation
          count: 1
```

</details>

## ğŸ› Troubleshooting

<div align="center">

### ğŸ”§ **Common Issues & Solutions**

*Quick fixes for the most frequent problems*

</div>

### â— **Installation Issues**

<details>
<summary>ğŸš« <strong>Model Not Found Error (Click to expand)</strong></summary>

```bash
# âŒ Error: "model 'mistral:7b' not found"

# âœ… Solution: Pull the model manually
docker exec agentic-ollama-main ollama pull mistral:7b

# ğŸ” Verify installation
docker exec agentic-ollama-main ollama list
```

**ğŸ¯ Alternative models if download fails:**
- `llama3.2:1b` (smaller, faster download)
- `phi3:mini` (Microsoft's efficient model)
- `gemma2:2b` (Google's lightweight model)

</details>

<details>
<summary>ğŸ’¾ <strong>Out of Memory Issues (Click to expand)</strong></summary>

```bash
# âŒ Error: "CUDA out of memory" or "OOM"

# âœ… Solution 1: Switch to smaller model
export LLM_MODEL="llama3.2:1b"
./deploy.sh main

# âœ… Solution 2: Reduce batch size
# Edit config/app_config.py
LLM_CONFIG = {
    "model": "llama3.2:1b",
    "max_tokens": 500,  # Reduce token limit
    "temperature": 0.7
}

# âœ… Solution 3: Use CPU-only mode
cd deployment
docker compose -f docker-compose.main-server.yml up -d
```

**ğŸ® GPU Memory Requirements:**
- **1B models**: 2GB VRAM
- **3B models**: 4GB VRAM  
- **7B models**: 8GB VRAM
- **8B+ models**: 12GB+ VRAM

</details>

<details>
<summary>ğŸ® <strong>GPU Not Detected (Click to expand)</strong></summary>

```bash
# âŒ Error: "NVIDIA-SMI has failed" or GPU not found

# âœ… Solution 1: Install NVIDIA Docker support
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# âœ… Solution 2: Verify GPU drivers
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi

# âœ… Solution 3: Check Docker GPU access
docker run --rm --gpus all ubuntu nvidia-smi
```

**ğŸ”§ GPU Setup Checklist:**
- [ ] NVIDIA drivers installed (470+)
- [ ] Docker with GPU support
- [ ] nvidia-docker2 package
- [ ] CUDA toolkit (optional but recommended)

</details>

### ğŸŒ **Network & Port Issues**

<details>
<summary>ğŸ”Œ <strong>Port Already in Use (Click to expand)</strong></summary>

```bash
# âŒ Error: "Port 8000 already in use"

# ğŸ” Check what's using the port
sudo netstat -tulpn | grep :8000
sudo lsof -i :8000

# âœ… Solution 1: Stop conflicting service
sudo kill -9 <PID>

# âœ… Solution 2: Use different ports
# Edit docker-compose files to use alternative ports
# 8001, 8002, etc.

# âœ… Solution 3: Clean up previous containers
docker ps -a
docker stop $(docker ps -aq)
docker rm $(docker ps -aq)
```

</details>

### ğŸ“Š **Performance Issues**

<details>
<summary>âš¡ <strong>Slow Response Times (Click to expand)</strong></summary>

**ğŸ¯ Optimization Strategies:**

1. **Model Selection:**
```bash
# Fast models for development
export LLM_MODEL="llama3.2:1b"  # ~2-3s response

# Balanced models for production  
export LLM_MODEL="mistral:7b"    # ~5-8s response
```

2. **GPU Acceleration:**
```bash
# Ensure GPU deployment
./deploy.sh main  # Uses GPU by default

# Check GPU utilization
nvidia-smi -l 1
```

3. **Resource Allocation:**
```yaml
# Increase Docker resources
deploy:
  resources:
    limits:
      memory: 32G
      cpus: '16'
```

</details>

### ğŸ” **Debugging & Monitoring**

```bash
# ğŸ“Š Check all containers
docker ps

# ğŸ“ View real-time logs
docker logs agentic-backend-main -f
docker logs agentic-ollama-main -f
docker logs agentic-frontend-main -f

# ğŸ’» System resource monitoring
docker stats

# ğŸ”§ Enter container for debugging
docker exec -it agentic-backend-main bash
docker exec -it agentic-ollama-main bash

# ğŸŒ Test API endpoints
curl http://localhost:8000/health
curl http://localhost:11434/api/tags
```

### ğŸ†˜ **Getting Help**

<div align="center">

| Issue Type | Solution | ğŸ”— |
|------------|----------|---|
| ğŸ› **Bugs** | [Create Issue](https://github.com/username/agentic-ai-project/issues) | ğŸ¯ |
| â“ **Questions** | [GitHub Discussions](https://github.com/username/agentic-ai-project/discussions) | ğŸ’¬ |
| ğŸ“– **Documentation** | [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) | ğŸ“‹ |
| ğŸ’¬ **Community** | [Discord/Slack](#) | ğŸ¤ |

</div>

## ğŸ“Š Performance

<div align="center">

### âš¡ **Optimization Guide**

*Get the best performance from your AI system*

</div>

### ğŸ¯ **Model Selection by Use Case**

| Use Case | Recommended Model | Memory | Speed | Quality |
|----------|------------------|--------|-------|---------|
| ğŸ§ª **Development/Testing** | `llama3.2:1b` | 4GB | âš¡âš¡âš¡âš¡âš¡ | â­â­â­ |
| ğŸ¢ **Production/General** | `mistral:7b` | 8GB | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ |
| ğŸ’» **Code Generation** | `codellama:7b` | 8GB | âš¡âš¡âš¡ | â­â­â­â­â­ |
| ğŸ“„ **Document Analysis** | `mistral:7b` | 8GB | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ |
| ğŸ¯ **High Quality** | `llama3.1:8b` | 12GB | âš¡âš¡âš¡ | â­â­â­â­â­ |

### ğŸ’» **Hardware Requirements**

<details>
<summary>ğŸ–¥ï¸ <strong>System Specifications (Click to expand)</strong></summary>

#### ğŸ® **GPU Recommendations**

| GPU Model | VRAM | Supported Models | Performance |
|-----------|------|------------------|-------------|
| **RTX 4090** | 24GB | All models | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **RTX 4080** | 16GB | Up to 8B | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **RTX 3080** | 10GB | Up to 7B | ğŸ”¥ğŸ”¥ğŸ”¥ |
| **RTX 3060** | 8GB | Up to 7B | ğŸ”¥ğŸ”¥ |
| **GTX 1660** | 6GB | Up to 3B | ğŸ”¥ |

#### ğŸ’¾ **Memory & Storage**

| Component | Minimum | Recommended | Optimal |
|-----------|---------|-------------|---------|
| **RAM** | 8GB | 16GB | 32GB+ |
| **Storage** | 20GB | 50GB | 100GB+ |
| **CPU** | 4 cores | 8 cores | 16+ cores |

</details>

### ğŸš€ **Performance Optimization Tips**

<details>
<summary>âš¡ <strong>Speed Optimization (Click to expand)</strong></summary>

#### ğŸ¯ **Model Configuration**
```python
# Fast response configuration
LLM_CONFIG = {
    "model": "llama3.2:1b",
    "temperature": 0.7,
    "max_tokens": 500,        # Shorter responses
    "top_p": 0.95,           # Focused sampling
    "repeat_penalty": 1.1    # Avoid repetition
}
```

#### ğŸ³ **Docker Optimization**
```yaml
# Resource allocation
deploy:
  resources:
    limits:
      memory: 16G
      cpus: '8'
    reservations:
      memory: 8G
      cpus: '4'
```

#### ğŸ”„ **Caching Strategies**
- Enable model caching in Ollama
- Use persistent volumes for model storage
- Implement response caching for repeated queries

</details>

### ğŸ“ˆ **Scaling Considerations**

<div align="center">

**ğŸŒ Deployment Scaling Guide**

</div>

| Users | Architecture | Hardware | Configuration |
|-------|-------------|----------|---------------|
| **1-5** | Single Server | 1x GPU, 16GB RAM | Standard deployment |
| **5-20** | Main + 3 Clients | 1x GPU, 32GB RAM | Load balancing |
| **20+** | Multi-GPU Cluster | 2+ GPUs, 64GB+ RAM | Kubernetes/Swarm |

## ğŸ¤ Contributing

<div align="center">

### ğŸŒŸ **Join Our Community!**

We welcome contributions from developers, researchers, and AI enthusiasts!

[![Contributors](https://img.shields.io/github/contributors/username/agentic-ai-project?style=for-the-badge&color=orange)](https://github.com/username/agentic-ai-project/graphs/contributors)
[![Pull Requests](https://img.shields.io/github/issues-pr/username/agentic-ai-project?style=for-the-badge&color=blue)](https://github.com/username/agentic-ai-project/pulls)

</div>

### ğŸš€ **How to Contribute**

<details>
<summary>ğŸ“ <strong>Contribution Guidelines (Click to expand)</strong></summary>

#### ğŸ¯ **Ways to Contribute**
- ğŸ› **Bug Reports**: Found a bug? [Create an issue](https://github.com/username/agentic-ai-project/issues/new)
- ğŸ’¡ **Feature Requests**: Have an idea? [Request a feature](https://github.com/username/agentic-ai-project/issues/new)
- ğŸ“– **Documentation**: Improve docs, add examples, fix typos
- ğŸ§ª **Testing**: Test new models, report compatibility issues
- ğŸ’» **Code**: Add new agents, tools, or optimizations

#### ğŸ”„ **Development Workflow**
1. ğŸ´ **Fork** the repository
2. ğŸŒ¿ **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ§ª **Test** with multiple models and configurations
4. ğŸ“ **Commit** your changes (`git commit -m 'Add amazing feature'`)
5. ğŸš€ **Push** to the branch (`git push origin feature/amazing-feature`)
6. ğŸ‰ **Submit** a pull request

#### ğŸ§ª **Testing Checklist**
- [ ] Test with different AI models (Mistral, Llama, CodeLlama)
- [ ] Verify GPU and CPU compatibility
- [ ] Check both single-server and distributed deployments
- [ ] Validate documentation updates
- [ ] Ensure code follows project style

</details>

### ğŸ† **Contributors Hall of Fame**

<div align="center">

*Thank you to all our amazing contributors!*

<!-- This will auto-populate with actual contributors when hosted on GitHub -->
<a href="https://github.com/username/agentic-ai-project/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=username/agentic-ai-project" />
</a>

**ğŸŒŸ Be the next contributor!**

</div>

## ğŸ“Š **Project Stats & Activity**

<div align="center">

![GitHub commit activity](https://img.shields.io/github/commit-activity/m/username/agentic-ai-project?style=for-the-badge&color=green)
![GitHub repo size](https://img.shields.io/github/repo-size/username/agentic-ai-project?style=for-the-badge&color=blue)
![Lines of code](https://img.shields.io/tokei/lines/github/username/agentic-ai-project?style=for-the-badge&color=orange)

</div>

## ğŸ“„ License

<div align="center">

### ğŸ“‹ **MIT License**

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**ğŸ‰ Free to use, modify, and distribute!**

</div>

---

<div align="center">

## ğŸš€ **Ready to Get Started?**

### **[âš¡ Quick Start](#-quick-start)** â€¢ **[ğŸ“– Documentation](#-table-of-contents)** â€¢ **[ğŸ¤ Contribute](#-contributing)**

---

### ğŸ’¬ **Support & Community**

ğŸ“– **Documentation**: [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)  
ğŸ› **Issues**: [GitHub Issues](https://github.com/username/agentic-ai-project/issues)  
ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/username/agentic-ai-project/discussions)  
ğŸ“§ **Contact**: [your-email@domain.com](mailto:your-email@domain.com)

---

### â­ **Show Your Support**

If this project helped you, please consider:
- â­ **Starring** the repository
- ğŸ› **Reporting** bugs and issues  
- ğŸ¤ **Contributing** to the codebase
- ğŸ“¢ **Sharing** with your network

**Built with â¤ï¸ by the open-source community**

*Last updated: June 2025*

</div>